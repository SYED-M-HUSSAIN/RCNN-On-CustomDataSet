# -*- coding: utf-8 -*-
"""CV_Q1,Q2(A)_ASSIGNMENT_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yHKQoZE5i7mBiwkABVICT4FjVjZBt6SL

# **QUESTION # 1 Theoretical Background**


Q1.1: Object detection is a computer vision technique that involves the identification and localization of objects in an image or a video. It is achieved by detecting the position of the object in the image and labeling it with a unique identifier. Object detection models use various algorithms and techniques, such as deep learning and computer vision, to identify objects and determine their locations within an image or video.

Q1.2: In object detection, the input consists of images and annotation files that provide the necessary information for object detection, such as object labels, bounding box coordinates, and image dimensions. The input data is usually divided into training and testing categories to build and evaluate the model's performance. The output of the object detection model consists of predicted bounding boxes and labels for each object in the input images.

Q1.3: Image recognition is a computer vision technique that involves identifying a single object in an image. The objective is to determine the class or category to which the object belongs. In contrast, object detection aims to identify and locate multiple objects in an image and assign labels to each of them.

Q1.4: Single stage detectors are a type of object detection algorithm that utilizes regression techniques to predict the bounding box coordinates and class probabilities for each object in an image. These models extract features from the input image and use them to identify objects and their locations. Examples of single stage detectors include YOLO (You Only Look Once) and SSD (Single Shot Detector).

Q1.5: Two stage detectors are a type of object detection algorithm that perform object detection in two stages. In the first stage, a region proposal network (RPN) is used to identify potential object locations in an image. In the second stage, a classification network is applied to these proposed regions of interest (ROIs) to determine the object class and bounding box coordinates. Examples of two stage detectors include Faster R-CNN (Region-based Convolutional Neural Network) and Mask R-CNN (Mask Region-based Convolutional Neural Network).

# **QUESTION # 2 RCNN Implementation**
### **PART A**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !unzip Images.zip
# !unzip Airplanes_Annotations.zip

"""### **DATASET EXPLANATION**
The dataset used in the tutorial is the NWPU-RESISC45 dataset, which contains 45 categories of remote sensing images. The dataset consists of over 31,000 images, with each image having a resolution of 256x256 pixels. The dataset also includes annotation files that provide information on the location and class of each object in the image. The tutorial uses this dataset to train an object detection model using the full R-CNN architecture in Keras. But we are using only the airport airplanes image data for this assignments training and testing which contain two folders Images and Airplanes_Annotations.
"""

import os,cv2,keras
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

path = "Images"
annot = "Airplanes_Annotations"
for e,i in enumerate(os.listdir(annot)):
    if e < 10:
        filename = i.split(".")[0]+".jpg"
        print(filename)
        img = cv2.imread(os.path.join(path,filename))
        df = pd.read_csv(os.path.join(annot,i))
        plt.imshow(img)
        for row in df.iterrows():
            x1 = int(row[1][0].split(" ")[0])
            y1 = int(row[1][0].split(" ")[1])
            x2 = int(row[1][0].split(" ")[2])
            y2 = int(row[1][0].split(" ")[3])
            cv2.rectangle(img,(x1,y1),(x2,y2),(255,0,0), 2)
        plt.figure()
        plt.imshow(img)
        break

"""**EXPLANATION**
This code reads in image files and their associated annotations from two directories, "Images" and "Airplanes_Annotations", respectively. For each annotation file, the code extracts the bounding box coordinates of the object of interest and draws a rectangle around it on the corresponding image. The resulting image is displayed using the matplotlib library. The "if e < 10" statement limits the loop to only the first 10 annotation files. The "break" statement ends the loop after the first iteration.
"""

cv2.setUseOptimized(True);
ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()

"""**EXPLANATION**
The code `cv2.setUseOptimized(True)` sets the flag for optimized code execution in OpenCV to True. This flag enables the use of optimized code paths in the OpenCV library that can significantly improve performance.

The code `ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()` creates an instance of the Selective Search algorithm for image segmentation, which is a common method for generating object proposals in object detection tasks. The `cv2.ximgproc.segmentation` module in OpenCV provides an implementation of this algorithm, and `createSelectiveSearchSegmentation()` is a method to create an object of the class that implements this algorithm. The `ss` object can then be used to generate object proposals for a given image.
"""

im = cv2.imread(os.path.join(path,"42850.jpg"))
ss.setBaseImage(im)
ss.switchToSelectiveSearchFast()
rects = ss.process()
imOut = im.copy()
for i, rect in (enumerate(rects)):
    x, y, w, h = rect
#     print(x,y,w,h)
#     imOut = imOut[x:x+w,y:y+h]
    cv2.rectangle(imOut, (x, y), (x+w, y+h), (0, 255, 0), 1, cv2.LINE_AA)
# plt.figure()
plt.imshow(imOut)
train_images=[]
train_labels=[]

def get_iou(bb1, bb2):
    assert bb1['x1'] < bb1['x2']
    assert bb1['y1'] < bb1['y2']
    assert bb2['x1'] < bb2['x2']
    assert bb2['y1'] < bb2['y2']

    x_left = max(bb1['x1'], bb2['x1'])
    y_top = max(bb1['y1'], bb2['y1'])
    x_right = min(bb1['x2'], bb2['x2'])
    y_bottom = min(bb1['y2'], bb2['y2'])

    if x_right < x_left or y_bottom < y_top:
        return 0.0

    intersection_area = (x_right - x_left) * (y_bottom - y_top)

    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])
    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])

    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)
    assert iou >= 0.0
    assert iou <= 1.0
    return iou

"""**EXPLANATION** This is a function that calculates the Intersection over Union (IoU) between two bounding boxes in terms of their top-left and bottom-right coordinates. The input arguments `bb1` and `bb2` are dictionaries with the keys `'x1'`, `'y1'`, `'x2'`, and `'y2'`, representing the top-left and bottom-right coordinates of the bounding boxes.

The function first checks if the bounding boxes are well-formed, i.e., that the x-coordinates of the top-left and bottom-right points are in the correct order and that the bounding boxes do not have zero area. It then calculates the intersection area of the two bounding boxes and their individual areas. Finally, it computes the IoU as the ratio of the intersection area to the union of the two bounding boxes. The function returns a scalar value between 0 and 1, where 0 indicates no overlap between the bounding boxes and 1 indicates complete overlap. The two assertions in the code ensure that the IoU value is within the valid range.

**LINK WHICH I STUDIED TO UNDERSTAND THIS CONCEPT IN DEPTH**
https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()
# for e,i in enumerate(os.listdir(annot)):
#     try:
#         if i.startswith("airplane"):
#             filename = i.split(".")[0]+".jpg"
#             if e==150:
#               break
# 
#             print(e,filename)
#             image = cv2.imread(os.path.join(path,filename))
#             df = pd.read_csv(os.path.join(annot,i))
#             gtvalues=[]
#             for row in df.iterrows():
#                 x1 = int(row[1][0].split(" ")[0])
#                 y1 = int(row[1][0].split(" ")[1])
#                 x2 = int(row[1][0].split(" ")[2])
#                 y2 = int(row[1][0].split(" ")[3])
#                 gtvalues.append({"x1":x1,"x2":x2,"y1":y1,"y2":y2})
#             ss.setBaseImage(image)
#             ss.switchToSelectiveSearchFast()
#             ssresults = ss.process()
#             imout = image.copy()
#             counter = 0
#             falsecounter = 0
#             flag = 0
#             fflag = 0
#             bflag = 0
#             for e,result in enumerate(ssresults):
#                 if e < 2000 and flag == 0:
#                     for gtval in gtvalues:
#                         x,y,w,h = result
#                         iou = get_iou(gtval,{"x1":x,"x2":x+w,"y1":y,"y2":y+h})
#                         if counter < 30:
#                             if iou > 0.70:
#                                 timage = imout[y:y+h,x:x+w]
#                                 resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER_AREA)
#                                 train_images.append(resized)
#                                 train_labels.append(1)
#                                 counter += 1
#                         else :
#                             fflag =1
#                         if falsecounter <30:
#                             if iou < 0.3:
#                                 timage = imout[y:y+h,x:x+w]
#                                 resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER_AREA)
#                                 train_images.append(resized)
#                                 train_labels.append(0)
#                                 falsecounter += 1
#                         else :
#                             bflag = 1
#                     if fflag == 1 and bflag == 1:
#                         print("inside")
#                         flag = 1
#     except Exception as e:
#         print(e)
#         print("error in "+filename)
#         continue

"""**EXPLANATION** This code is iterating through all the annotation files in the directory and selecting the ones that contain the word "airplane" in their filename. For each of these files, it reads the corresponding image and annotation data. It then uses selective search to generate potential regions of interest in the image and computes the intersection over union (IoU) between each potential region and the ground truth bounding boxes. Regions with an IoU greater than 0.7 are considered true positives and regions with an IoU less than 0.3 are considered false positives. A maximum of 30 true positives and 30 false positives are added to the training set for each image. If more than 30 true positives or false positives are found, it stops adding to the training set.

**DRAWBACK** I made modification in it after discussing with TA and  peers and reduce the iteration process to 150 images otherwise after that program crashes due to RAM issue in colab notebook.
"""

X_new = np.array(train_images)
y_new = np.array(train_labels)
X_new.shape

from keras.layers import Dense
from keras import Model
from keras import optimizers
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.vgg16 import VGG16

vggmodel = VGG16(weights='imagenet', include_top=True)
vggmodel.summary()

for layers in (vggmodel.layers)[:15]:
    print(layers)
    layers.trainable = False
X= vggmodel.layers[-2].output
predictions = Dense(2, activation="softmax")(X)
model_final = Model(vggmodel.input, predictions)
from keras.optimizers import Adam
opt = Adam(lr=0.0001)
model_final.compile(loss = keras.losses.categorical_crossentropy, optimizer = opt, metrics=["accuracy"])
model_final.summary()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
class MyLabelBinarizer(LabelBinarizer):
    def transform(self, y):
        Y = super().transform(y)
        if self.y_type_ == 'binary':
            return np.hstack((Y, 1-Y))
        else:
            return Y
    def inverse_transform(self, Y, threshold=None):
        if self.y_type_ == 'binary':
            return super().inverse_transform(Y[:, 0], threshold)
        else:
            return super().inverse_transform(Y, threshold)
lenc = MyLabelBinarizer()
Y =  lenc.fit_transform(y_new)
X_train, X_test , y_train, y_test = train_test_split(X_new,Y,test_size=0.10)
print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

trdata = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)
traindata = trdata.flow(x=X_train, y=y_train)
tsdata = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)
testdata = tsdata.flow(x=X_test, y=y_test)

from keras.callbacks import ModelCheckpoint, EarlyStopping
checkpoint = ModelCheckpoint("ieeercnn_vgg16_1.h5", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)
early = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')
hist = model_final.fit_generator(generator= traindata, steps_per_epoch= 10, epochs= 100, validation_data= testdata, validation_steps=2, callbacks=[checkpoint,early])

import matplotlib.pyplot as plt
# plt.plot(hist.history["acc"])
# plt.plot(hist.history['val_acc'])
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title("model loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend(["Loss","Validation Loss"])
plt.show()
plt.savefig('chart loss.png')

im = X_test[190]
plt.imshow(im)
img = np.expand_dims(im, axis=0)
out= model_final.predict(img)
if out[0][0] > out[0][1]:
    print("plane")
else:
    print("not plane")

z=0
for e,i in enumerate(os.listdir(path)):
    if i.startswith("4"):
        z += 1
        img = cv2.imread(os.path.join(path,i))
        ss.setBaseImage(img)
        ss.switchToSelectiveSearchFast()
        ssresults = ss.process()
        imout = img.copy()
        for e,result in enumerate(ssresults):
            if e < 2000:
                x,y,w,h = result
                timage = imout[y:y+h,x:x+w]
                resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER_AREA)
                img = np.expand_dims(resized, axis=0)
                out= model_final.predict(img)
                if out[0][0] > 0.65:
                    cv2.rectangle(imout, (x, y), (x+w, y+h), (0, 255, 0), 1, cv2.LINE_AA)
        plt.figure()
        plt.imshow(imout)

"""**OVER ALL PERFORMANCE**
### **loss: 0.1051 - accuracy: 0.9531 - val_loss: 0.2215 - val_accuracy: 0.9531**
 The overall performance of the model is quite good with a training accuracy of 0.9531 and a validation accuracy of 0.9531. The loss value for the training dataset is 0.1051 and for the validation dataset is 0.2215. The model is likely not overfitting as the validation accuracy is similar to the training accuracy, which is a good sign. It is possible that using more annotated images could improve the performance of the model. The more training data available, the better the model can learn to recognize patterns and generalize to new examples. With only 150 annotated images, the model may not have seen enough variation in the data to be able to generalize well to new, unseen images. Additionally, the GPU error may have prevented the model from training for as many epochs as desired, which can also limit its performance. We tested above this model with test data and its working correctly.
"""